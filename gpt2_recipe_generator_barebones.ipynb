{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Generation with GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook explores how to **fine-tune a transformer** for a custom text completion task -- specifically, we'll teach GPT-2 to generate full recipe instructions from just a list of ingredients.\n",
    "\n",
    "Before the world met ChatGPT, GPT-2 was already showing us what generative AI could do -- if we were paying attention. To learn more about this large language model, which was **trained on 8 million web pages**, check out the official model card on [Hugging Face](https://huggingface.co/gpt2).\n",
    "\n",
    "Before we get cooking, here's a roadmap of what is to come:\n",
    "- Set up the GPU runtime\n",
    "- Download and inspect the dataset\n",
    "- Create a DataFrame and split it into training and test sets\n",
    "- Test the base model on a single sentence\n",
    "- Fine-tune the model using the recipe dataset\n",
    "- Evaluate the fine-tuned model's recipe-writing skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU time"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check which GPU is available for model training\n",
    "!nvidia-smi -L"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install, if needed, and import the Hugging Face Transformers library\n",
    "!pip install transformers\n",
    "import transformers"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install gdown, a Python tool that helps you download files from Google Drive directly into your Colab\n",
    "!pip install --upgrade gdown"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset includes 120K recipes! All the recipes are formatted in the same way, and, as we will see, formatting is critical for solving text completion tasks like this. Part of the magic of transformers is that they are able to recognize and learn patterns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gdown\n",
    "# Download the recipe dataset (120K examples) from Google Drive\n",
    "gdrivelink='https://drive.google.com/uc?id=10KF1LqW9k2MgTb1GwSPlfvX1INFPDxi5'\n",
    "gdown.download(gdrivelink, quiet=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recipe dataset at  https://eightportions.com/datasets/Recipes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create the DataFrame that we just downloaded\n",
    "df = pd.read_csv('recipes.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the first five rows\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the lenght\n",
    "len(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Now check the shape\n",
    "df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check for null values\n",
    "df.isna().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to explore the `Combined` column -- the only column that matters in this text generation task that we are going to have GPT-2 complete."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 'Combined' is the only column we care about in this exercise\n",
    "# It's the combined ingredients and instructions and it's essential that we format the training set vert carefully\n",
    "df.combined"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Let's improve the formatting by printing the column and selecting .iloc[0], or just the first row\n",
    "# Important note: So, this is the format for every element in this column\n",
    "print(df.combined.iloc[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: Every set of Instructions ends with the following -- <|endoftext|>. This is crucial because the model needs to know where in the training set a sentence ends.\n",
    "\n",
    "Also, notice that ingredients in the text often appear in the same order in the Ingredients section *and* the Instructions section. Humans can notice this pattern, of course, and transformers can, too.\n",
    "\n",
    "So, here's **the gameplan**: We will show GPT-2 120K examples where the Ingredients section is followed by the Instructions section. Then we will provide it just the Ingredients section and it will be prompted to generate text, or Instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do you measure the performance of generating text?\n",
    "\n",
    "It's a little murky.\n",
    "\n",
    "Even if there is not a clear metric we can use, we will still split the dataset into two -- training and test."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create the training set, just the 'combined' column values and grab the first 120,000\n",
    "dataset_train = df.combined.values[:120000]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the length\n",
    "len(dataset_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# For the test set, we will take the rest\n",
    "dataset_test = df.combined.values[120000:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the length\n",
    "len(dataset_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, we have a huge training set. But we need to turn the training and text sets into **text files**. And this is why those <|endoftext|> tokens are so important."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Here's how to create the text file for 'dataset_train'\n",
    "with open('dataset_train.txt', 'w') as f:\n",
    "  f.write('\\n'.join(dataset_train))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# And now for the test set\n",
    "with open('dataset_test.txt', 'w') as f:\n",
    "  f.write('\\n'.join(dataset_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# These are massive files, so, let's just display a small piece of 'dataset_train.txt'\n",
    "!head -20 dataset_train.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pretrained GPT-2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to load the core ingredients: the **pretrained GPT-2 model** and its **tokenizer**. We'll use Hugging Face's `transformers` library to grab both.\n",
    "\n",
    "Specifically, we\u2019ll be using:\n",
    "1. `AutoModelForCausalLM` -- the GPT-2 language model for text generation\n",
    "2. `AutoTokenizer` -- the tokenizer that breaks text into tokens the model understands"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Here you can either use your own model or download one from Hugging Face\n",
    "# We, of course, will be using GPT-2 but note: The distiled version, which is small, fast\n",
    "# and still almost as accurate as the famous original\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively, load a fine-tuned model we previously saved"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gdown\n",
    "gdrivelink='https://drive.google.com/drive/folders/1qHEQ6zpOeGDiBlZ3q4zkpMVSVa86d69q?usp=sharing'\n",
    "gdown.download_folder(gdrivelink, quiet=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('recipe_generation_model')\n",
    "model = AutoModelForCausalLM.from_pretrained('recipe_generation_model')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model on one sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's find out how the pretrained model does on the task of completing a sentence.\n",
    "\n",
    "We\u2019ll first encode a sentence using the tokenizer and see what the model sees.\n",
    "\n",
    "**Side Note**: When we use `return_tensors='pt'`, we're actually pulling in **PyTorch** under the hood. Even though we didn\u2019t explicitly import it, Hugging Face\u2019s `transformers` library is built on top of PyTorch by default.\n",
    ">\n",
    "So, the output is a PyTorch tensor  -- a powerful data structure that we\u2019ll work with more as we go deeper into model building."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# First, define the input text\n",
    "# And then encode it with 'tokenizer.encode()'\n",
    "input_text = 'I hit the slopes early to snowboard, caught the first lift, and by noon I had already'\n",
    "enc_input = tokenizer.encode(input_text, return_tensors='pt', add_special_tokens=False)\n",
    "enc_input"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized our input sentence, we can feed it into the model and generate predictions to complete the sentence.\n",
    "\n",
    "Note: This block of code is a **beast** so make sure to read through those comment lines."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "output_sequences = model.generate(\n",
    "    input_ids = enc_input, # The encoded input from earlier\n",
    "    max_length= 70,  # The max length of the generated sentence\n",
    "    temperature = 0.9, # Controls randomness, closer to 1 = more creative, closer to 0 = more predictable\n",
    "    top_k = 20, # Considers only the top 20 most likely next words\n",
    "    top_p = 0.9, # Allows dynamic cutoff of word options based on cumulative probabilioty\n",
    "    repetition_penalty = 1, # Penalty for repeating a word in the input\n",
    "    do_sample = True, # If true, this allows for randomness instead of always picking the highest-probability word\n",
    "    num_return_sequences = 5 # Number of output sentences\n",
    ")\n",
    "for i in range(len(output_sequences)):\n",
    "  print(f'{i}: {tokenizer.decode(output_sequences[i])}\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, these outputs are -- mostly, if not enitrely -- nonsense. But what about a recipe? Can are model accomplish that?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display the first element from 'dataset_train'\n",
    "dataset_train[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Need to split the above output and add that 'Instructions' column as a prompt\n",
    "dataset_train[0].split('Instructions:')[0]+'Instructions:'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Now, let's try it as our new input text\n",
    "# Note: Need to increase the length of the generated sentence\n",
    "new_input_text = dataset_train[0].split('Instructions:')[0]+'Instructions:'\n",
    "enc_input = tokenizer.encode(new_input_text, return_tensors='pt', add_special_tokens=False)\n",
    "enc_input\n",
    "output_sequences = model.generate(\n",
    "    input_ids = enc_input, # The encoded input from earlier\n",
    "    max_length= 150,  # The max length of the generated sentence\n",
    "    temperature = 0.9, # Controls randomness, closer to 1 = more creative, closer to 0 = more predictable\n",
    "    top_k = 20, # Considers only the top 20 most likely next words\n",
    "    top_p = 0.9, # Allows dynamic cutoff of word options based on cumulative probabilioty\n",
    "    repetition_penalty = 1, # Penalty for repeating a word in the input\n",
    "    do_sample = True, # If true, this allows for randomness instead of always picking the highest-probability word\n",
    "    num_return_sequences = 5 # Number of output sentences\n",
    ")\n",
    "for i in range(len(output_sequences)):\n",
    "  print(f'{i}: {tokenizer.decode(output_sequences[i])}\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results aren't a total mess -- well, at least the ones that aren't left blank.\n",
    "\n",
    "GPT-2 is speaking gramatically correct English but still isn't accomplishing the task we set out to do. So, up next? Fine-tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin fine-tuning, we\u2019ll download a lightweight training script provided by Michele Samorani, Associate Professor, Information Systems & Analytics at the Leavey School of Business at Santa Clara Univeristy.\n",
    "\n",
    "The script is adapted from Hugging Face\u2019s official examples. it handles the model training loop and saves outputs to a local `experiments/` folder.\n",
    "\n",
    "The original Hugging Face version can be found [here](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Download the custom fine-tuning script from the course server\n",
    "!curl https://webpages.scu.edu/ftp/msamorani/NLP/run_lm_finetuning.py > run_lm_finetuning.py"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a folder to store training outputs and model checkpoints\n",
    "!mkdir experiments"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set a couple of parameters\n",
    "# First, epochs set to three so the model will go through the dataset three times\n",
    "epochs = 3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Then find the file that has our training set\n",
    "file_with_training_set = 'dataset_train.txt'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create the bash scipt to fine-tune the language model\n",
    "# This loops through the number of epochs and build a command that:\n",
    "# 1. Calls the training script\n",
    "# 2. Points to the input file\n",
    "# 3. Saves model checkpoints to a directory named by epoch\n",
    "# 4. Uses distilGPT2 and basic Hugging Face flags\n",
    "text = f\"for epoch in {epochs} \\n\"+\\\n",
    "\"do \\n\"+\\\n",
    "\"python run_lm_finetuning.py \"+\\\n",
    "f\"--output_dir=experiments/epoch_{epochs} \"+\\\n",
    "\"--model_type=gpt2 \"+\\\n",
    "\"--model_name_or_path=distilgpt2 \"+\\\n",
    "f\"--train_data_file={file_with_training_set} \"+\\\n",
    "\"--do_train \"+\\\n",
    "\"--overwrite_output_dir \"+\\\n",
    "\"--save_steps=500 \" +\\\n",
    "f\"--num_train_epochs={epochs} \\n\" +\\\n",
    "\"done\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the fine-tuning bash script to a file called 'run_experiments.sh'\n",
    "# This will be executed in the next cell to launch the training loops\n",
    "f = open('run_experiments.sh',mode='w')\n",
    "f.write(text)\n",
    "f.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the core training step where the GPT-2 model learns from our dataset using the training script we generated earlier. This loop runs for 3 epochs, saving checkpoints at defined intervals.\n",
    "\n",
    "**Note:** Due to time and GPU constraints, I manually interrupted training after about 25 minutes, or 17% of total steps. Despite the early stop, several useful checkpoints (500, 1000, 1500) were created and will be used in the next step."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Launch the fine-tuning process\n",
    "# Note: This was manually stopped after about 25 minutes at iteration 1700+\n",
    "!bash run_experiments.sh"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the available checkpoints\n",
    "!ls experiments/epoch_3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this step, we save our fine-tuned language model using Hugging Face\u2019s `AutoModelForCausalLM` and `AutoTokenizer` classes.\n",
    "\n",
    "- The model and tokenizer are loaded from the last available checkpoint, or `checkpoint-1500`\n",
    "- Then we save them to a new folder: `recipe_generation_model`\n",
    "- Finally, we mount Google Drive and copy the model over for long-term storage and later use"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the model + tokenizer from the last checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained('experiments/epoch_3/checkpoint-1500')\n",
    "model = AutoModelForCausalLM.from_pretrained('experiments/epoch_3/checkpoint-1500')\n",
    "\n",
    "# Define a folder name for the saved model\n",
    "model_path = 'recipe_generation_model'\n",
    "\n",
    "# And save the model and tokenizer locally\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Mount Google Drive so we can export our fine-tuned model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# And copy the saved model to your Google Drive\n",
    "import shutil\n",
    "shutil.copytree(model_path, '/content/drive/MyDrive/' + model_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the fine-tuned model's recipe writing skills"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Begin by grabbing the test set\n",
    "print(dataset_test[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Need to split at 'Instructions' which will be our prompt\n",
    "print(dataset_test[0].split('Instructions:')[0] + '\\n Instructions')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Now store it as input_text_test\n",
    "input_text_test = dataset_test[0].split('Instructions:')[0] + '\\n Instructions'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input_text_test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take 2 with the recipe generation..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# And now to test out the model\n",
    "input_text_test = dataset_train[0].split('Instructions:')[0]+'Instructions:'\n",
    "enc_input = tokenizer.encode(input_text_test, return_tensors='pt', add_special_tokens=False)\n",
    "enc_input\n",
    "output_sequences = model.generate(\n",
    "    input_ids = enc_input, # The encoded input from earlier\n",
    "    max_length= 300,  # The max length of the generated sentence\n",
    "    temperature = 0.9, # Controls randomness, closer to 1 = more creative, closer to 0 = more predictable\n",
    "    top_k = 20, # Considers only the top 20 most likely next words\n",
    "    top_p = 0.9, # Allows dynamic cutoff of word options based on cumulative probabilioty\n",
    "    repetition_penalty = 1, # Penalty for repeating a word in the input\n",
    "    do_sample = True, # If true, this allows for randomness instead of always picking the highest-probability word\n",
    "    num_return_sequences = 5 # Number of output sentences\n",
    ")\n",
    "for i in range(len(output_sequences)):\n",
    "  print(f'{i}: {tokenizer.decode(output_sequences[i])}\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at that! These instructions actually make sense. You could cook these meals.\n",
    "\n",
    "Just for fun, let's see if the model can handle that **snowboarding prompt** from before."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Let's quickly update the model\n",
    "input_text_snowboarding = 'I hit the slopes early to snowboard, caught the first lift, and by noon I had already'\n",
    "enc_input = tokenizer.encode(input_text_snowboarding, return_tensors='pt', add_special_tokens=False)\n",
    "enc_input\n",
    "output_sequences = model.generate(\n",
    "    input_ids = enc_input, # The encoded input from earlier\n",
    "    max_length= 70,  # The max length of the generated sentence\n",
    "    temperature = 0.9, # Controls randomness, closer to 1 = more creative, closer to 0 = more predictable\n",
    "    top_k = 20, # Considers only the top 20 most likely next words\n",
    "    top_p = 0.9, # Allows dynamic cutoff of word options based on cumulative probabilioty\n",
    "    repetition_penalty = 1, # Penalty for repeating a word in the input\n",
    "    do_sample = True, # If true, this allows for randomness instead of always picking the highest-probability word\n",
    "    num_return_sequences = 5 # Number of output sentences\n",
    "    )\n",
    "for i in range(len(output_sequences)):\n",
    "  print(f'{i}: {tokenizer.decode(output_sequences[i])}\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final check: model analysis after fine-tuning**\n",
    "\n",
    "The output above confirms that fine-tuning worked as intended: the model can now generate coherent recipe instructions, but fails to respond meaningfully to off-domain prompts -- like snowboarding. This is a sign of successful domain specialization. Our GPT-2 model has learned to focus on recipe generation, not general language tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}